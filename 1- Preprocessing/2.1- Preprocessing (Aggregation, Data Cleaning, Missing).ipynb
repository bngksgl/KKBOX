{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We started with the reduction of the transaction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_transaction=pd.read_csv(\"transactions.csv\")\n",
    "data_train=pd.read_csv(\"train_csv\")\n",
    "#First we are inner joining train dataset with our transaction dataset\n",
    "merged_train=pd.merge(data_train, data_transaction, on='msno')\n",
    "#Second in order to reduce the dataset even more we decided to take last six months transactions\n",
    "reduced_merged_train=merged_train[merged_train['transaction_date']>20160831]\n",
    "#saving the dataset just in case\n",
    "pd.to_csv('transaction_reduced.csv')\n",
    "#Third, we applied the aggregate functions to reduce the dataset in a way that each line represents one customer\n",
    "df_transactions_sample= pd.read_csv('transaction_reduced.csv')\n",
    "def transaction_statistics(df):\n",
    "\n",
    "    # step1: sort \n",
    "    df_sorted = df.sort_values(['transaction_date'],ascending=True)  \n",
    "    # remind: .sort_values will change dtypes into float\n",
    "\n",
    "    # step2: construct new 'transaction' with new varailables\n",
    "    newDF = {}\n",
    "   \n",
    "    newDF['avg_plan_list_price']=int(df_sorted['plan_list_price'].mean())\n",
    "    newDF['avg_actual_amount_paid']=int(df_sorted['actual_amount_paid'].mean())\n",
    "    newDF['total_cancel']=int(df_sorted['is_cancel'].sum())\n",
    "    newDF['most_fq_payment_method_id']=int(df_sorted['payment_method_id'].value_counts().idxmax())\n",
    "    newDF['most_frq_payment_plan_days']=int(df_sorted['payment_plan_days'].value_counts().idxmax())\n",
    "  \n",
    "    newDF['msno']=df_sorted.iloc[0]['msno']\n",
    "    newDF['is_auto_renew']=int(df_sorted.iloc[-1]['is_auto_renew'])\n",
    "    newDF['first_transaction_date']=int(df_sorted.iloc[0]['transaction_date'])\n",
    "    newDF['last_expiration_date']=int(df_sorted.iloc[-1]['membership_expire_date'])\n",
    "    newDF['total_churn']=total_churn(df)\n",
    "    \n",
    "     #compute number of active days for the customer\n",
    "    first_activity=str(newDF['first_transaction_date'])\n",
    "    last_activity=str(newDF['last_expiration_date'])\n",
    "    first_activity_datetime=datetime.datetime.strptime(first_activity,'%Y%m%d')\n",
    "    last_activity_datetime=datetime.datetime.strptime(last_activity,'%Y%m%d')\n",
    "    active_days=last_activity_datetime-first_activity_datetime\n",
    "    \n",
    "    newDF['active_days']=active_days.days\n",
    "    return newDF\n",
    "\n",
    "# computing the number of times a customer has churned based on his transaction history\n",
    "#input df = the relevant rows of one customer \n",
    "import datetime\n",
    "\n",
    "def total_churn(df):\n",
    "\n",
    "    df['index_col'] = range(0, len(df))\n",
    "    df['index_col']\n",
    "    i = 1 \n",
    "    count = len(df)  \n",
    "    total_churn = 0\n",
    "    df['index_col'] = df.index\n",
    "\n",
    "    while i < count:\n",
    "        \n",
    "        cur_tra_date = df.iloc[i]['transaction_date'].astype('str')\n",
    "        cur_tra_datetime = datetime.datetime.strptime(cur_tra_date, '%Y%m%d')\n",
    "        \n",
    "        pre_exp_date = df.iloc[i-1]['membership_expire_date'].astype('str')\n",
    "        pre_exp_datetime = datetime.datetime.strptime(pre_exp_date, '%Y%m%d')\n",
    "\n",
    "        churn_days = cur_tra_datetime- pre_exp_datetime\n",
    "        #print(churn_days.days)\n",
    "    \n",
    "    \n",
    "        if churn_days.days > 30:\n",
    "            total_churn += 1\n",
    "    \n",
    "        i += 1\n",
    "\n",
    "    return total_churn\n",
    "\n",
    "\n",
    "### remove noisy data\n",
    "def de_noisy(df):\n",
    "    # total remove 200k\n",
    "    ds_nosiy = df.query('payment_plan_days<1 or plan_list_price<1 or actual_amount_paid<1')\n",
    "    #ds_nosiy = df[(df['payment_plan_days']<1) | (df['plan_list_price']<1) | (df['actual_amount_paid']<1)]\n",
    "    df_wo_noisy = df.drop(ds_nosiy.index)\n",
    "    return df_wo_noisy\n",
    "\n",
    "\n",
    "#de_noisy(df_transactions_sample[0:200]).head(140)\n",
    "\n",
    "\n",
    "# ### final output in DataFrame\n",
    "transaction_new=de_noisy(df_transactions_sample).groupby('msno').apply(transaction_statistics) # output three individual Dictionary! \n",
    "print(\"Group By is done\")\n",
    "\n",
    "# integrate all customer dictionaries into one super dic.\n",
    "super_dic = {}\n",
    "for c in transaction_new:\n",
    "    for k, v in c.items():  # d.items() in Python 3+\n",
    "        super_dic.setdefault(k, []).append(v)\n",
    "        \n",
    "# show as a Table (DataFrame)\n",
    "super_transaction = pd.DataFrame(super_dic,columns=super_dic.keys())\n",
    "super_transaction.reset_index(level=0, inplace=True)\n",
    "super_transaction.to_csv(\"transactions_done.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UserLog Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we started with the reduction of the userlogs\n",
    "userlogs=pd.read_csv(\"userlogs.csv\")\n",
    "#This time first we reduced the dataset\n",
    "reduced_userlogs=userlogs[userlogs['date']>20160831]\n",
    "#Second inner joined it with train dataset\n",
    "data_train=pd.read_csv(\"train_csv\")\n",
    "userlogs_reduced=pd.merge(data_train, reduced_userlogs, on='msno')\n",
    "#Here we saved it just in case\n",
    "userlogs_reduced.to_csv(\"userlog_reduced.csv\")\n",
    "#Third, we applied the aggregate functions to reduce the dataset in a way that each line represents one customer.\n",
    "#Also we created specific variables that will represent specific months before the churn\n",
    "csv=pd.read_csv(\"userlog_reduced.csv\", dtype={'date':object})\n",
    "csv.head()\n",
    "csv.describe()\n",
    "csv.dtypes\n",
    "\n",
    "#subsettig year, month, day from the date variable and making the date numeric\n",
    "i=0\n",
    "nrows=csv.shape[0]\n",
    "x = np.zeros((nrows,4))\n",
    "while(i<nrows):\n",
    "    s=csv['date'][i]\n",
    "    x[i,0]=int(s[0:4])\n",
    "    x[i,1]=int(s[4:6])\n",
    "    x[i,2]=int(s[6:8])\n",
    "    x[i,3]=int(s[0:8])\n",
    "    i=i+1\n",
    "csv=csv.assign(year=x[:,0],month=x[:,1],day=x[:,2],date_numeric=x[:,3])\n",
    "print(csv.head())\n",
    "print(csv.dtypes)\n",
    "print(\"Month,day is DONE\")\n",
    "\n",
    "#creating sum, mean, median, max and min of variables\n",
    "def f(x):\n",
    "    d = {}\n",
    "    d['num_25_sum'] = x['num_25'].sum()\n",
    "    d['num_50_sum'] = x['num_50'].sum()\n",
    "    d['num_75_sum'] = x['num_75'].sum()\n",
    "    d['num_985_sum'] = x['num_985'].sum()\n",
    "    d['num_100_sum'] = x['num_100'].sum()\n",
    "    d['num_unq_sum'] = x['num_unq'].sum()\n",
    "    d['num_totalsec_sum'] = x['total_secs'].sum()\n",
    "    d['num_25_mean'] = x['num_25'].mean()\n",
    "    d['num_50_mean'] = x['num_50'].mean()\n",
    "    d['num_75_mean'] = x['num_75'].mean()\n",
    "    d['num_985_mean'] = x['num_985'].mean()\n",
    "    d['num_100_mean'] = x['num_100'].mean()\n",
    "    d['num_unq_mean'] = x['num_unq'].mean()\n",
    "    d['num_totalsec_mean'] = x['total_secs'].mean()\n",
    "    d['num_25_med'] = x['num_25'].median()\n",
    "    d['num_50_med'] = x['num_50'].median()\n",
    "    d['num_75_med'] = x['num_75'].median()\n",
    "    d['num_985_med'] = x['num_985'].median()\n",
    "    d['num_100_med'] = x['num_100'].median()\n",
    "    d['num_unq_med'] = x['num_unq'].median()\n",
    "    d['num_totalsec_med'] = x['total_secs'].median()\n",
    "    d['num_25_max'] = x['num_25'].max()\n",
    "    d['num_50_max'] = x['num_50'].max()\n",
    "    d['num_75_max'] = x['num_75'].max()\n",
    "    d['num_985_max'] = x['num_985'].max()\n",
    "    d['num_100_max'] = x['num_100'].max()\n",
    "    d['num_unq_max'] = x['num_unq'].max()\n",
    "    d['num_totalsec_max'] = x['total_secs'].max()\n",
    "    d['num_25_min'] = x['num_25'].min()\n",
    "    d['num_50_min'] = x['num_50'].min()\n",
    "    d['num_75_min'] = x['num_75'].min()\n",
    "    d['num_985_min'] = x['num_985'].min()\n",
    "    d['num_100_min'] = x['num_100'].min()\n",
    "    d['num_unq_min'] = x['num_unq'].min()\n",
    "    d['num_totalsec_min'] = x['total_secs'].min()\n",
    "    d['number_of_days_listened'] = x['date'].count()\n",
    "    return pd.Series(d, index=[\"num_25_sum\",\"num_50_sum\",\"num_75_sum\",\"num_985_sum\",\"num_100_sum\",\"num_unq_sum\",\"num_totalsec_sum\",\"num_25_mean\",\"num_50_mean\",\"num_75_mean\",\"num_985_mean\",\"num_100_mean\",\"num_unq_mean\",\"num_totalsec_mean\",\"num_25_med\",\"num_50_med\",\"num_75_med\",\"num_985_med\",\"num_100_med\",\"num_unq_med\",\"num_totalsec_med\",\"num_25_max\",\"num_50_max\",\"num_75_max\",\"num_985_max\",\"num_100_max\",\"num_unq_max\",\"num_totalsec_max\",\"num_25_min\",\"num_50_min\",\"num_75_min\",\"num_985_min\",\"num_100_min\",\"num_unq_min\",\"num_totalsec_min\",\"number_of_days_listened\"])\n",
    "data=csv.groupby('msno').apply(f)\n",
    "\n",
    "# resetting the index\n",
    "\n",
    "data.reset_index(level=0, inplace=True)\n",
    "\n",
    "#Calculating churn variables\n",
    "\n",
    "#how many times one person has listened in 201702 specific month. If not it should be zero\n",
    "def f2017_02(x):\n",
    "    d = {}\n",
    "    d['num_25_201702_sum'] = x['num_25'].sum()\n",
    "    d['num_50_201702_sum'] = x['num_50'].sum()\n",
    "    d['num_75_201702_sum'] = x['num_75'].sum()\n",
    "    d['num_985_201702_sum'] = x['num_985'].sum()\n",
    "    d['num_100_201702_sum'] = x['num_100'].sum()\n",
    "    d['num_unq_201702_sum'] = x['num_unq'].sum()\n",
    "    d['num_totalsec_201702_sum'] = x['total_secs'].sum()\n",
    "    d['num_25_201702_mean'] = x['num_25'].mean()\n",
    "    d['num_50_201702_mean'] = x['num_50'].mean()\n",
    "    d['num_75_201702_mean'] = x['num_75'].mean()\n",
    "    d['num_985_201702_mean'] = x['num_985'].mean()\n",
    "    d['num_100_201702_mean'] = x['num_100'].mean()\n",
    "    d['num_unq_201702_mean'] = x['num_unq'].mean()\n",
    "    d['num_totalsec_201702_mean'] = x['total_secs'].mean()\n",
    "    d['number_of_days_201702_listened'] = x['date'].count()\n",
    "    return pd.Series(d, index=[\"num_25_201702_sum\",\"num_50_201702_sum\",\"num_75_201702_sum\",\"num_985_201702_sum\",\"num_100_201702_sum\",\"num_unq_201702_sum\",\"num_totalsec_201702_sum\",\"num_25_201702_mean\",\"num_50_201702_mean\",\"num_75_201702_mean\",\"num_985_201702_mean\",\"num_100_201702_mean\",\"num_unq_201702_mean\",\"num_totalsec_201702_mean\",\"number_of_days_201702_listened\"])\n",
    "#how many times one person has listened in 201701 specific month. If not it should be zero\n",
    "def f2017_01(x):\n",
    "    d = {}\n",
    "    d['num_25_201701_sum'] = x['num_25'].sum()\n",
    "    d['num_50_201701_sum'] = x['num_50'].sum()\n",
    "    d['num_75_201701_sum'] = x['num_75'].sum()\n",
    "    d['num_985_201701_sum'] = x['num_985'].sum()\n",
    "    d['num_100_201701_sum'] = x['num_100'].sum()\n",
    "    d['num_unq_201701_sum'] = x['num_unq'].sum()\n",
    "    d['num_totalsec_201701_sum'] = x['total_secs'].sum()\n",
    "    d['num_25_201701_mean'] = x['num_25'].mean()\n",
    "    d['num_50_201701_mean'] = x['num_50'].mean()\n",
    "    d['num_75_201701_mean'] = x['num_75'].mean()\n",
    "    d['num_985_201701_mean'] = x['num_985'].mean()\n",
    "    d['num_100_201701_mean'] = x['num_100'].mean()\n",
    "    d['num_unq_201701_mean'] = x['num_unq'].mean()\n",
    "    d['num_totalsec_201701_mean'] = x['total_secs'].mean()\n",
    "    d['number_of_days_201701_listened'] = x['date'].count()\n",
    "    return pd.Series(d, index=[\"num_25_201701_sum\",\"num_50_201701_sum\",\"num_75_201701_sum\",\"num_985_201701_sum\",\"num_100_201701_sum\",\"num_unq_201701_sum\",\"num_totalsec_201701_sum\",\"num_25_201701_mean\",\"num_50_201701_mean\",\"num_75_201701_mean\",\"num_985_201701_mean\",\"num_100_201701_mean\",\"num_unq_201701_mean\",\"num_totalsec_201701_mean\",\"number_of_days_201701_listened\"])\n",
    "#how many times one person has listened in 201612 specific month. If not it should be zero\n",
    "def f2016_12(x):\n",
    "    d = {}\n",
    "    d['num_25_201612_sum'] = x['num_25'].sum()\n",
    "    d['num_50_201612_sum'] = x['num_50'].sum()\n",
    "    d['num_75_201612_sum'] = x['num_75'].sum()\n",
    "    d['num_985_201612_sum'] = x['num_985'].sum()\n",
    "    d['num_100_201612_sum'] = x['num_100'].sum()\n",
    "    d['num_unq_201612_sum'] = x['num_unq'].sum()\n",
    "    d['num_totalsec_201612_sum'] = x['total_secs'].sum()\n",
    "    d['num_25_201612_mean'] = x['num_25'].mean()\n",
    "    d['num_50_201612_mean'] = x['num_50'].mean()\n",
    "    d['num_75_201612_mean'] = x['num_75'].mean()\n",
    "    d['num_985_201612_mean'] = x['num_985'].mean()\n",
    "    d['num_100_201612_mean'] = x['num_100'].mean()\n",
    "    d['num_unq_201612_mean'] = x['num_unq'].mean()\n",
    "    d['num_totalsec_201612_mean'] = x['total_secs'].mean()\n",
    "    d['number_of_days_201612_listened'] = x['date'].count()\n",
    "    return pd.Series(d, index=[\"num_25_201612_sum\",\"num_50_201612_sum\",\"num_75_201612_sum\",\"num_985_201612_sum\",\"num_100_201612_sum\",\"num_unq_201612_sum\",\"num_totalsec_201612_sum\",\"num_25_201612_mean\",\"num_50_201612_mean\",\"num_75_201612_mean\",\"num_985_201612_mean\",\"num_100_201612_mean\",\"num_unq_201612_mean\",\"num_totalsec_201612_mean\",\"number_of_days_201612_listened\"])\n",
    "#applying the function\n",
    "data201702=csv[(csv['year']==2017) & (csv['month'] == 2)].groupby('msno').apply(f2017_02)\n",
    "data201701=csv[(csv['year']==2017) & (csv['month'] == 1)].groupby('msno').apply(f2017_01)\n",
    "data201612=csv[(csv['year']==2016) & (csv['month'] == 12)].groupby('msno').apply(f2016_12)\n",
    "#fixing the indexes\n",
    "data201702.reset_index(level=0, inplace=True)\n",
    "data201701.reset_index(level=0, inplace=True)\n",
    "data201612.reset_index(level=0, inplace=True)\n",
    "\n",
    "result=data.merge(data201702,on='msno',how='left').merge(data201701,on='msno',how='left').merge(data201612,on='msno',how='left')\n",
    "print(result.head())\n",
    "\n",
    "# Summing up the months of february and january \n",
    "def flasttwomonths(x):\n",
    "    d = {}\n",
    "    d['num_25_lasttwo_sum'] = x['num_25'].sum()\n",
    "    d['num_50_lasttwo_sum'] = x['num_50'].sum()\n",
    "    d['num_75_lasttwo_sum'] = x['num_75'].sum()\n",
    "    d['num_985_lasttwo_sum'] = x['num_985'].sum()\n",
    "    d['num_100_lasttwo_sum'] = x['num_100'].sum()\n",
    "    d['num_unq_lasttwo_sum'] = x['num_unq'].sum()\n",
    "    d['num_totalsec_lasttwo_sum'] = x['total_secs'].sum()\n",
    "    d['num_25_lasttwo_mean'] = x['num_25'].mean()\n",
    "    d['num_50_lasttwo_mean'] = x['num_50'].mean()\n",
    "    d['num_75_lasttwo_mean'] = x['num_75'].mean()\n",
    "    d['num_985_lasttwo_mean'] = x['num_985'].mean()\n",
    "    d['num_100_lasttwo_mean'] = x['num_100'].mean()\n",
    "    d['num_unq_lasttwo_mean'] = x['num_unq'].mean()\n",
    "    d['num_totalsec_lasttwo_mean'] = x['total_secs'].mean()\n",
    "    d['number_of_days_lasttwo_listened'] = x['date'].count()\n",
    "    return pd.Series(d, index=[\"num_25_lasttwo_sum\",\"num_50_lasttwo_sum\",\"num_75_lasttwo_sum\",\"num_985_lasttwo_sum\",\"num_100_lasttwo_sum\",\"num_unq_lasttwo_sum\",\"num_totalsec_lasttwo_sum\",\"num_25_lasttwo_mean\",\"num_50_lasttwo_mean\",\"num_75_lasttwo_mean\",\"num_985_lasttwo_mean\",\"num_100_lasttwo_mean\",\"num_unq_lasttwo_mean\",\"num_totalsec_lasttwo_mean\",\"number_of_days_lasttwo_listened\"])\n",
    "\n",
    "#summing up the months of january february and december\n",
    "def flastthreemonths(x):\n",
    "    d = {}\n",
    "    d['num_25_lastthree_sum'] = x['num_25'].sum()\n",
    "    d['num_50_lastthree_sum'] = x['num_50'].sum()\n",
    "    d['num_75_lastthree_sum'] = x['num_75'].sum()\n",
    "    d['num_985_lastthree_sum'] = x['num_985'].sum()\n",
    "    d['num_100_lastthree_sum'] = x['num_100'].sum()\n",
    "    d['num_unq_lastthree_sum'] = x['num_unq'].sum()\n",
    "    d['num_totalsec_lastthree_sum'] = x['total_secs'].sum()\n",
    "    d['num_25_lastthree_mean'] = x['num_25'].mean()\n",
    "    d['num_50_lastthree_mean'] = x['num_50'].mean()\n",
    "    d['num_75_lastthree_mean'] = x['num_75'].mean()\n",
    "    d['num_985_lastthree_mean'] = x['num_985'].mean()\n",
    "    d['num_100_lastthree_mean'] = x['num_100'].mean()\n",
    "    d['num_unq_lastthree_mean'] = x['num_unq'].mean()\n",
    "    d['num_totalsec_lastthree_mean'] = x['total_secs'].mean()\n",
    "    d['number_of_days_lastthree_listened'] = x['date'].count()\n",
    "    return pd.Series(d, index=[\"num_25_lastthree_sum\",\"num_50_lastthree_sum\",\"num_75_lastthree_sum\",\"num_985_lastthree_sum\",\"num_100_lastthree_sum\",\"num_unq_lastthree_sum\",\"num_totalsec_lastthree_sum\",\"num_25_lastthree_mean\",\"num_50_lastthree_mean\",\"num_75_lastthree_mean\",\"num_985_lastthree_mean\",\"num_100_lastthree_mean\",\"num_unq_lastthree_mean\",\"num_totalsec_lastthree_mean\",\"number_of_days_lastthree_listened\"])\n",
    "\n",
    "#applying the function\n",
    "#last two months calculation (feb+january)\n",
    "datalasttwomonths=csv[(csv['date_numeric']<20170301) & (csv['date_numeric'] > 20161230)].groupby('msno').apply(flasttwomonths)\n",
    "#last three months calculation (feb+january+december)\n",
    "datalastthreemonths=csv[(csv['date_numeric']<20170301) & (csv['date_numeric'] > 20161130)].groupby('msno').apply(flastthreemonths)\n",
    "\n",
    "#fixing the indexes\n",
    "datalasttwomonths.reset_index(level=0, inplace=True)\n",
    "datalastthreemonths.reset_index(level=0, inplace=True)\n",
    "\n",
    "result=result.merge(datalasttwomonths,on='msno',how='left').merge(datalastthreemonths,on='msno',how='left')\n",
    "print(result.head())\n",
    "print(result.dtypes)\n",
    "\n",
    "#fixing the empty columns. Because the if the customer hasn't listened any song within the last three months, the technique we use creates null columns \n",
    "result=result.fillna(0)\n",
    "print(\"Non empty columns are fixed\")\n",
    "result.to_csv(\"user_logs_done.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Merging the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merging all the datasets\n",
    "#We realized that not all the customers that are in the train dataset are in the member dataset. Therefore, first we decided to inner join members with \n",
    "#train. After joining transaction and member datasets, we are going to left join the merged dataset with transaction and userlogs datasets.\n",
    "userlogs=pd.read_csv(\"user_logs_done.csv\")\n",
    "member=pd.read_csv(\"members.csv\")\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "transaction=pd.read_csv(\"transactions_done.csv\")\n",
    "member_train=pd.merge(member,train, on='msno')\n",
    "mem_tra_userog=pd.merge(member_train,userlogs, on='msno', how='left')\n",
    "all=pd.merge(mem_tra_userog,transaction, on='msno', how='left')\n",
    "#looking at the last dataset\n",
    "all.shape\n",
    "all.dtypes\n",
    "all.describe()\n",
    "#saving the data\n",
    "all.to_csv(\"last_data_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Handling for Userlog Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Missing Handling-> filling variables that came from userlog with 0\n",
    "all=pd.read_csv(\"last_data_v1.csv\")\n",
    "all[\"num_25_sum\"]=all[\"num_25_sum\"].fillna(0)\n",
    "all[\"num_50_sum\"]=all[\"num_50_sum\"].fillna(0)\n",
    "all[\"num_75_sum\"]=all[\"num_75_sum\"].fillna(0)\n",
    "all[\"num_985_sum\"]=all[\"num_985_sum\"].fillna(0)\n",
    "all[\"num_100_sum\"]=all[\"num_100_sum\"].fillna(0)\n",
    "all[\"num_unq_sum\"]=all[\"num_unq_sum\"].fillna(0)\n",
    "all[\"num_totalsec_sum\"]=all[\"num_totalsec_sum\"].fillna(0)\n",
    "all[\"num_25_mean\"]=all[\"num_25_mean\"].fillna(0)\n",
    "all[\"num_50_mean\"]=all[\"num_50_mean\"].fillna(0)\n",
    "all[\"num_75_mean\"]=all[\"num_75_mean\"].fillna(0)\n",
    "all[\"num_985_mean\"]=all[\"num_985_mean\"].fillna(0)\n",
    "all[\"num_100_mean\"]=all[\"num_100_mean\"].fillna(0)\n",
    "all[\"num_unq_mean\"]=all[\"num_unq_mean\"].fillna(0)\n",
    "all[\"num_totalsec_mean\"]=all[\"num_totalsec_mean\"].fillna(0)\n",
    "all[\"num_25_med\"]=all[\"num_25_med\"].fillna(0)\n",
    "all[\"num_50_med\"]=all[\"num_50_med\"].fillna(0)\n",
    "all[\"num_75_med\"]=all[\"num_75_med\"].fillna(0)\n",
    "all[\"num_985_med\"]=all[\"num_985_med\"].fillna(0)\n",
    "all[\"num_100_med\"]=all[\"num_100_med\"].fillna(0)\n",
    "all[\"num_unq_med\"]=all[\"num_unq_med\"].fillna(0)\n",
    "all[\"num_totalsec_med\"]=all[\"num_totalsec_med\"].fillna(0)\n",
    "all[\"num_25_max\"]=all[\"num_25_max\"].fillna(0)\n",
    "all[\"num_50_max\"]=all[\"num_50_max\"].fillna(0)\n",
    "all[\"num_75_max\"]=all[\"num_75_max\"].fillna(0)\n",
    "all[\"num_985_max\"]=all[\"num_985_max\"].fillna(0)\n",
    "all[\"num_100_max\"]=all[\"num_100_max\"].fillna(0)\n",
    "all[\"num_unq_max\"]=all[\"num_unq_max\"].fillna(0)\n",
    "all[\"num_totalsec_max\"]=all[\"num_totalsec_max\"].fillna(0)\n",
    "all[\"num_25_min\"]=all[\"num_25_min\"].fillna(0)\n",
    "all[\"num_50_min\"]=all[\"num_50_min\"].fillna(0)\n",
    "all[\"num_75_min\"]=all[\"num_75_min\"].fillna(0)\n",
    "all[\"num_985_min\"]=all[\"num_985_min\"].fillna(0)\n",
    "all[\"num_100_min\"]=all[\"num_100_min\"].fillna(0)\n",
    "all[\"num_unq_min\"]=all[\"num_unq_min\"].fillna(0)\n",
    "all[\"num_totalsec_min\"]=all[\"num_totalsec_min\"].fillna(0)\n",
    "all[\"number_of_days_listened\"]=all[\"number_of_days_listened\"].fillna(0)\n",
    "all[\"num_25_201702_sum\"]=all[\"num_25_201702_sum\"].fillna(0)\n",
    "all[\"num_50_201702_sum\"]=all[\"num_50_201702_sum\"].fillna(0)\n",
    "all[\"num_75_201702_sum\"]=all[\"num_75_201702_sum\"].fillna(0)\n",
    "all[\"num_985_201702_sum\"]=all[\"num_985_201702_sum\"].fillna(0)\n",
    "all[\"num_100_201702_sum\"]=all[\"num_100_201702_sum\"].fillna(0)\n",
    "all[\"num_unq_201702_sum\"]=all[\"num_unq_201702_sum\"].fillna(0)\n",
    "all[\"num_totalsec_201702_sum\"]=all[\"num_totalsec_201702_sum\"].fillna(0)\n",
    "all[\"num_25_201702_mean\"]=all[\"num_25_201702_mean\"].fillna(0)\n",
    "all[\"num_50_201702_mean\"]=all[\"num_50_201702_mean\"].fillna(0)\n",
    "all[\"num_75_201702_mean\"]=all[\"num_75_201702_mean\"].fillna(0)\n",
    "all[\"num_985_201702_mean\"]=all[\"num_985_201702_mean\"].fillna(0)\n",
    "all[\"num_100_201702_mean\"]=all[\"num_100_201702_mean\"].fillna(0)\n",
    "all[\"num_unq_201702_mean\"]=all[\"num_unq_201702_mean\"].fillna(0)\n",
    "all[\"num_totalsec_201702_mean\"]=all[\"num_totalsec_201702_mean\"].fillna(0)\n",
    "all[\"number_of_days_201702_listened\"]=all[\"number_of_days_201702_listened\"].fillna(0)\n",
    "all[\"num_25_201701_sum\"]=all[\"num_25_201701_sum\"].fillna(0)\n",
    "all[\"num_50_201701_sum\"]=all[\"num_50_201701_sum\"].fillna(0)\n",
    "all[\"num_75_201701_sum\"]=all[\"num_75_201701_sum\"].fillna(0)\n",
    "all[\"num_985_201701_sum\"]=all[\"num_985_201701_sum\"].fillna(0)\n",
    "all[\"num_100_201701_sum\"]=all[\"num_100_201701_sum\"].fillna(0)\n",
    "all[\"num_unq_201701_sum\"]=all[\"num_unq_201701_sum\"].fillna(0)\n",
    "all[\"num_totalsec_201701_sum\"]=all[\"num_totalsec_201701_sum\"].fillna(0)\n",
    "all[\"num_25_201701_mean\"]=all[\"num_25_201701_mean\"].fillna(0)\n",
    "all[\"num_50_201701_mean\"]=all[\"num_50_201701_mean\"].fillna(0)\n",
    "all[\"num_75_201701_mean\"]=all[\"num_75_201701_mean\"].fillna(0)\n",
    "all[\"num_985_201701_mean\"]=all[\"num_985_201701_mean\"].fillna(0)\n",
    "all[\"num_100_201701_mean\"]=all[\"num_100_201701_mean\"].fillna(0)\n",
    "all[\"num_unq_201701_mean\"]=all[\"num_unq_201701_mean\"].fillna(0)\n",
    "all[\"num_totalsec_201701_mean\"]=all[\"num_totalsec_201701_mean\"].fillna(0)\n",
    "all[\"number_of_days_201701_listened\"]=all[\"number_of_days_201701_listened\"].fillna(0)\n",
    "all[\"num_25_201612_sum\"]=all[\"num_25_201612_sum\"].fillna(0)\n",
    "all[\"num_50_201612_sum\"]=all[\"num_50_201612_sum\"].fillna(0)\n",
    "all[\"num_75_201612_sum\"]=all[\"num_75_201612_sum\"].fillna(0)\n",
    "all[\"num_985_201612_sum\"]=all[\"num_985_201612_sum\"].fillna(0)\n",
    "all[\"num_100_201612_sum\"]=all[\"num_100_201612_sum\"].fillna(0)\n",
    "all[\"num_unq_201612_sum\"]=all[\"num_unq_201612_sum\"].fillna(0)\n",
    "all[\"num_totalsec_201612_sum\"]=all[\"num_totalsec_201612_sum\"].fillna(0)\n",
    "all[\"num_25_201612_mean\"]=all[\"num_25_201612_mean\"].fillna(0)\n",
    "all[\"num_50_201612_mean\"]=all[\"num_50_201612_mean\"].fillna(0)\n",
    "all[\"num_75_201612_mean\"]=all[\"num_75_201612_mean\"].fillna(0)\n",
    "all[\"num_985_201612_mean\"]=all[\"num_985_201612_mean\"].fillna(0)\n",
    "all[\"num_100_201612_mean\"]=all[\"num_100_201612_mean\"].fillna(0)\n",
    "all[\"num_unq_201612_mean\"]=all[\"num_unq_201612_mean\"].fillna(0)\n",
    "all[\"num_totalsec_201612_mean\"]=all[\"num_totalsec_201612_mean\"].fillna(0)\n",
    "all[\"number_of_days_201612_listened\"]=all[\"number_of_days_201612_listened\"].fillna(0)\n",
    "all[\"num_25_lasttwo_sum\"]=all[\"num_25_lasttwo_sum\"].fillna(0)\n",
    "all[\"num_50_lasttwo_sum\"]=all[\"num_50_lasttwo_sum\"].fillna(0)\n",
    "all[\"num_75_lasttwo_sum\"]=all[\"num_75_lasttwo_sum\"].fillna(0)\n",
    "all[\"num_985_lasttwo_sum\"]=all[\"num_985_lasttwo_sum\"].fillna(0)\n",
    "all[\"num_100_lasttwo_sum\"]=all[\"num_100_lasttwo_sum\"].fillna(0)\n",
    "all[\"num_unq_lasttwo_sum\"]=all[\"num_unq_lasttwo_sum\"].fillna(0)\n",
    "all[\"num_totalsec_lasttwo_sum\"]=all[\"num_totalsec_lasttwo_sum\"].fillna(0)\n",
    "all[\"num_25_lasttwo_mean\"]=all[\"num_25_lasttwo_mean\"].fillna(0)\n",
    "all[\"num_50_lasttwo_mean\"]=all[\"num_50_lasttwo_mean\"].fillna(0)\n",
    "all[\"num_75_lasttwo_mean\"]=all[\"num_75_lasttwo_mean\"].fillna(0)\n",
    "all[\"num_985_lasttwo_mean\"]=all[\"num_985_lasttwo_mean\"].fillna(0)\n",
    "all[\"num_100_lasttwo_mean\"]=all[\"num_100_lasttwo_mean\"].fillna(0)\n",
    "all[\"num_unq_lasttwo_mean\"]=all[\"num_unq_lasttwo_mean\"].fillna(0)\n",
    "all[\"num_totalsec_lasttwo_mean\"]=all[\"num_totalsec_lasttwo_mean\"].fillna(0)\n",
    "all[\"number_of_days_lasttwo_listened\"]=all[\"number_of_days_lasttwo_listened\"].fillna(0)\n",
    "all[\"num_25_lastthree_sum\"]=all[\"num_25_lastthree_sum\"].fillna(0)\n",
    "all[\"num_50_lastthree_sum\"]=all[\"num_50_lastthree_sum\"].fillna(0)\n",
    "all[\"num_75_lastthree_sum\"]=all[\"num_75_lastthree_sum\"].fillna(0)\n",
    "all[\"num_985_lastthree_sum\"]=all[\"num_985_lastthree_sum\"].fillna(0)\n",
    "all[\"num_100_lastthree_sum\"]=all[\"num_100_lastthree_sum\"].fillna(0)\n",
    "all[\"num_unq_lastthree_sum\"]=all[\"num_unq_lastthree_sum\"].fillna(0)\n",
    "all[\"num_totalsec_lastthree_sum\"]=all[\"num_totalsec_lastthree_sum\"].fillna(0)\n",
    "all[\"num_25_lastthree_mean\"]=all[\"num_25_lastthree_mean\"].fillna(0)\n",
    "all[\"num_50_lastthree_mean\"]=all[\"num_50_lastthree_mean\"].fillna(0)\n",
    "all[\"num_75_lastthree_mean\"]=all[\"num_75_lastthree_mean\"].fillna(0)\n",
    "all[\"num_985_lastthree_mean\"]=all[\"num_985_lastthree_mean\"].fillna(0)\n",
    "all[\"num_100_lastthree_mean\"]=all[\"num_100_lastthree_mean\"].fillna(0)\n",
    "all[\"num_unq_lastthree_mean\"]=all[\"num_unq_lastthree_mean\"].fillna(0)\n",
    "all[\"num_totalsec_lastthree_mean\"]=all[\"num_totalsec_lastthree_mean\"].fillna(0)\n",
    "all[\"number_of_days_lastthree_listened\"]=all[\"number_of_days_lastthree_listened\"].fillna(0)\n",
    "#dropping one unrelated variable\n",
    "all=all.drop([\"Unnamed: 0_y\",\"index\"],axis=1)\n",
    "#saving it again\n",
    "all.to_csv(\"last_data_v3.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Handling & Data Cleaning for Transaction Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Missing Handling Continues\n",
    "data=pd.read_csv(\"last_data_v3.csv\")\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "#checking the dataset\n",
    "print(data['num_totalsec_max'].min())\n",
    "#data=data.drop['Unnamed: 0_y','index','first_transaction_date','last_expiration_date']\n",
    "#handling missing values with gender\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data['gender'] = LabelEncoder().fit_transform(data['gender'].astype(str)) #male==1, NAN==2,female==0\n",
    "data['gender'] = LabelEncoder().fit_transform(data['gender'].astype(np.int8))\n",
    "\n",
    "# find most 'frequent paymen_method_id' and 'most_frq_payment_plan_days' of non-null data\n",
    "most_freq_paymethod=data['most_fq_payment_method_id'].value_counts().idxmax()\n",
    "most_freq_payplandays=data['most_frq_payment_plan_days'].value_counts().idxmax()\n",
    "most_freq_autorenew=data['is_auto_renew'].value_counts().idxmax()\n",
    "most_freq_totcancel=data['total_cancel'].value_counts().idxmax()\n",
    "most_freq_planlist=data['avg_plan_list_price'].value_counts().idxmax()\n",
    "most_freq_actamount=data['avg_actual_amount_paid'].value_counts().idxmax()\n",
    "print(most_freq_paymethod)\n",
    "print(most_freq_payplandays)\n",
    "print(most_freq_autorenew)\n",
    "print(most_freq_totcancel)\n",
    "print(most_freq_planlist)\n",
    "print(most_freq_actamount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['most_fq_payment_method_id']=data['most_fq_payment_method_id'].fillna(most_freq_paymethod) #41\n",
    "data['most_frq_payment_plan_days']=data['most_frq_payment_plan_days'].fillna(most_freq_payplandays) #30\n",
    "data['total_churn']=data['total_churn'].fillna(0)\n",
    "data['is_auto_renew']=data['is_auto_renew'].fillna(most_freq_autorenew) #1\n",
    "data['total_cancel']=data['total_cancel'].fillna(most_freq_totcancel) #0\n",
    "data['active_days']=data['active_days'].fillna(0)\n",
    "data['avg_plan_list_price']=data['avg_plan_list_price'].fillna(most_freq_planlist) #149\n",
    "data['avg_actual_amount_paid']=data['avg_actual_amount_paid'].fillna(most_freq_actamount) #149\n",
    "\n",
    "#create new variable\n",
    "#calculate payment diference: \n",
    "data['payment_different']=data['avg_actual_amount_paid']-data['avg_plan_list_price']\n",
    "data['num_below_50_sum'] = data['num_25_sum'] + data['num_50_sum'] \n",
    "data['num_above_50_sum'] = data['num_75_sum'] + data['num_985_sum']+data['num_100_sum']\n",
    "data['total_songs_listened']=data['num_25_sum'] + data['num_50_sum']+ data['num_75_sum'] + data['num_985_sum']+data['num_100_sum']\n",
    "data['proportion_songs_above_50']=data['num_above_50_sum']/data['total_songs_listened']\n",
    "\n",
    "#Even though we handled with missing values, due to the fact that the denomater has 0 values, python couldn't do the division and created missing values\n",
    "data['proportion_songs_above_50']=data['proportion_songs_above_50'].fillna(0)\n",
    "\n",
    "#dtypes getting the whole list\n",
    "data_dtypes=data.dtypes\n",
    "data_dtypes.to_csv(\"dtypes_v3.csv\")\n",
    "\n",
    "#dropping unrelated variables\n",
    "data=data.drop([\"Unnamed: 0\",\"Unnamed: 0.1\",\"Unnamed: 0_x\",\"registration_init_time\",\"expiration_date\",\"first_transaction_date\",\"last_expiration_date\"],axis=1)\n",
    "\n",
    "#checking missingness-- No miissing values found.\n",
    "i=0\n",
    "n=data.shape[1]\n",
    "column=data.columns\n",
    "while(i<n):\n",
    "    value=data[column[i]].isnull().values.any()\n",
    "    if(value==True):\n",
    "        print(column[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Changing seconds to hours\n",
    "data[\"num_totalsec_sum\"]=data[\"num_totalsec_sum\"]/3600\n",
    "data[\"num_totalsec_mean\"]=data[\"num_totalsec_mean\"]/3600\n",
    "data[\"num_totalsec_med\"]=data[\"num_totalsec_med\"]/3600\n",
    "data[\"num_totalsec_max\"]=data[\"num_totalsec_max\"]/3600\n",
    "data[\"num_totalsec_min\"]=data[\"num_totalsec_min\"]/3600\n",
    "data[\"num_totalsec_201702_sum\"]=data[\"num_totalsec_201702_sum\"]/3600\n",
    "data[\"num_totalsec_201702_mean\"]=data[\"num_totalsec_201702_mean\"]/3600\n",
    "data[\"num_totalsec_201701_sum\"]=data[\"num_totalsec_201701_sum\"]/3600\n",
    "data[\"num_totalsec_201701_mean\"]=data[\"num_totalsec_201701_mean\"]/3600\n",
    "data[\"num_totalsec_201612_sum\"]=data[\"num_totalsec_201612_sum\"]/3600\n",
    "data[\"num_totalsec_201612_mean\"]=data[\"num_totalsec_201612_mean\"]/3600\n",
    "data[\"num_totalsec_lasttwo_sum\"]=data[\"num_totalsec_lasttwo_sum\"]/3600\n",
    "data[\"num_totalsec_lasttwo_mean\"]=data[\"num_totalsec_lasttwo_mean\"]/3600\n",
    "data[\"num_totalsec_lastthree_sum\"]=data[\"num_totalsec_lastthree_sum\"]/3600\n",
    "data[\"num_totalsec_lastthree_mean\"]=data[\"num_totalsec_lastthree_mean\"]/3600\n",
    "\n",
    "\n",
    "#checking maximum values\n",
    "data_desc=data.describe()\n",
    "data_desc.to_csv(\"data_desc_v3.csv\")\n",
    "\n",
    "#looking at the mean value of age\n",
    "age_normal = data.query(' 0<bd<81')\n",
    "print(age_normal.shape)\n",
    "print(age_normal['bd'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#handling outliers in age\n",
    "data.ix[data.bd <0, 'bd'] = 0\n",
    "data.ix[(data.bd >0) & (data.bd < 10) , 'bd'] = 29.74059164332968\n",
    "data.ix[data.bd >80, 'bd'] = 29.74059164332968\n",
    "\n",
    "#Active_days handling\n",
    "data.loc[(data.active_days > -10) & (data.active_days < 0), 'active_days'] = 0\n",
    "data=data.drop(data[data.active_days < -10].index)  # drop exreme ourlier value\n",
    "data.to_csv(\"last_data_v5_prioroutliers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
